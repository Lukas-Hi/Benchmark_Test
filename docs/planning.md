# Planning: Entscheider-Benchmark
**HID-LINKEDIN-BENCHMARK-2026-02-06-ACTIVE-C4E8A1-CLO46**
Stand: 6. Februar 2026

---

## 1. Ursprung und Motivation

### AuslÃ¶ser
Am 6. Februar 2026 (Tag des Opus 4.6 Release) entstand die Idee, einen eigenen Benchmark zu entwickeln, der misst, was wissenschaftliche Benchmarks nicht messen: Wie gut unterstÃ¼tzen LLMs reale strategische Entscheidungen?

### LÃ¼cke
Existierende Benchmarks (MMLU, HumanEval, GPQA, etc.) testen akademisches Wissen, Coding-FÃ¤higkeit, logisches Denken. Kein Benchmark testet, ob ein LLM einem GF mit 45 Mitarbeitern eine brauchbare Entscheidungsvorlage liefert. GDPval (OpenAI, Oktober 2025) kommt am nÃ¤chsten â€“ 44 Berufe, echte Arbeitsprodukte â€“ aber deckt strategische FÃ¼hrungsarbeit im KMU-Kontext nicht ab.

### Positionierung
Der Benchmark dient gleichzeitig als Content-Instrument (LinkedIn) und als Ã¶ffentliches Tool (GitHub-Repository). Die Community-Komponente erlaubt es anderen, den Benchmark zu forken und eigene Ergebnisse beizutragen.

---

## 2. Entscheidungslog

### 2.1 Aufgaben-Design
**Entscheidung:** Sechs Aufgabenkategorien, alle auf strategische FÃ¼hrungsarbeit fokussiert.
**BegrÃ¼ndung:** Operative Aufgaben (E-Mails verbessern, Texte schreiben) sind fÃ¼r die Thoughtful-Leader-Positionierung zu kleinteilig.
**Verworfene Alternative:** Breiter Mix aus operativen + strategischen Aufgaben.

| ID | Aufgabe | Was wird getestet |
|----|---------|-------------------|
| A1 | Entscheidungsvorlage | Risikoerkennung, fehlende Info, kritische Distanz |
| A2 | Strategische Zusammenfassung | Signal/Rauschen trennen, KMU-Kontextualisierung |
| A3 | Kritisches Hinterfragen | Strukturelle SchwÃ¤chen erkennen, konstruktive Kritik |
| A4 | Szenario-Analyse | Distinkte Szenarien, Annahmen explizit machen |
| A5 | Widerspruchserkennung | Quellenvalidierung, Ursachen fÃ¼r Diskrepanzen |
| A6 | Zahlenanalyse | Korrekte Extraktion, nicht-offensichtliche Muster |

### 2.2 Zwei Prompt-Varianten (N/P)
**Entscheidung:** Jede Aufgabe in zwei Varianten â€“ Normal-User und Power-User.
**BegrÃ¼ndung:** Der Delta zwischen N und P misst den Wert von Prompt-Kompetenz. Das ist die zentrale These des Benchmarks und der Content-Strategie: â€žKompetenz im Umgang mit KI macht den Unterschied, nicht das Tool."
- **N (Normal):** Kein System-Prompt, kein Kontext-Setup, Frage wie ein GF sie stellen wÃ¼rde.
- **P (Power):** Voller System-Prompt, KONTEXT â†’ SITUATION â†’ AUFTRAG, Guardrails, explizite Anforderungen.

### 2.3 Multi-Provider statt nur OpenRouter
**Entscheidung:** Direkt-APIs fÃ¼r Abo-Modelle (Anthropic, OpenAI, Google), OpenRouter nur fÃ¼r Rest.
**BegrÃ¼ndung:** Gerald hat Pro/Max/Business-Abos. 13 von 18 Modellen laufen Ã¼ber Abos = keine Zusatzkosten. Nur 5 Ã¼ber OpenRouter. GeschÃ¤tzte Ersparnis: 50â€“100 EUR pro Durchlauf.
**Verworfene Alternative:** Alles Ã¼ber OpenRouter (einfacher, aber teurer).

### 2.4 10 Runs bei Temperatur 0
**Entscheidung:** 10 Wiederholungen pro ModellÃ—Aufgabe, Temperatur 0.
**BegrÃ¼ndung:** Artificial Analysis nutzt >10 Wiederholungen fÃ¼r 95%-Konfidenzintervall. GDPval fÃ¤hrt 1 Run (statistische Absicherung Ã¼ber Aufgabenmenge). 10 Runs ist der Kompromiss zwischen Aussagekraft und Kosten. Temperatur 0 fÃ¼r maximale Reproduzierbarkeit.
**Alternative:** 3 Runs (schneller, billiger, aber schwÃ¤chere Statistik).

### 2.5 Modellauswahl
**Entscheidung:** 18 Modelle in 4 Kategorien.
**BegrÃ¼ndung:** Frontier zeigt State of the Art, Mid-Tier zeigt Preis-Leistung, Coding-Modelle testen ob Spezialisierung bei Strategie schadet, Reasoning-Modelle testen ob explizites Denken hilft.

| Kategorie | Modelle | Warum |
|-----------|---------|-------|
| Frontier (7) | Opus 4.6, Opus 4.5, GPT-5.2, GPT-5.2 Pro, Gemini 3 Pro, Gemini 2.5 Pro, Grok 4.1 | BestmÃ¶gliche Leistung pro Anbieter |
| Mid-Tier (7) | Sonnet 4.5, Haiku 4.5, GPT-5.2 Chat, Gemini Flash, Mistral Large 3, DeepSeek V3.2, Llama 3.3 70B | Preis-Leistung fÃ¼r KMU |
| Coding (1) | GPT-5.2-Codex | Spezialisierung vs. Generalismus |
| Reasoning (2) | DeepSeek R1, o1 | Explizites Denken bei strategischen Aufgaben |

**Hinweis:** GPT-5.3-Codex (Release 5. Feb 2026) fehlt â€“ API noch nicht Ã¼ber OpenRouter verfÃ¼gbar. Wird manuell Ã¼ber VS Code nachgetestet.

### 2.6 Quelldokumente
**Entscheidung:** 3 Aufgaben szenariobasiert (kein Dokument nÃ¶tig), 3 dokumentbasiert.

| Aufgabe | Dokument | Status |
|---------|----------|--------|
| A1, A3, A4 | â€“ (Szenario im Prompt) | âœ“ Fertig |
| A2 | BCG AI Radar 2026 Executive Summary | â¬œ Zu beschaffen |
| A5_N | Alan Turing Framework (88S.) + EU AI Act (144S.) â€“ volle PDFs | âœ“ Fertig |
| A5_P | Kuratierte Extrakte (~2.5k Tokens) aus beiden Dokumenten | âœ“ Fertig |
| A6 | EVN GeschÃ¤ftsbericht 2024/25 | âœ“ Fertig |

---

## 3. Roadmap

### Phase 1: Infrastruktur âœ“
- [x] Benchmark-Methodik definiert
- [x] Aufgaben und Prompts formuliert (6Ã—N + 6Ã—P = 12)
- [x] Multi-Provider Runner implementiert (benchmark.py)
- [x] Projektstruktur und Dokumentation angelegt
- [x] CLAUDE.md, planning.md, specs.md, methodology.md, scoring_guide.md
- [x] Logik-Check: Keine Fehler, 9 Warnungen dokumentiert
- [x] Benchmark-Specs Skill erstellt (benchmark-specs.skill)
- [x] Refactoring: Monolith (810Z) â†’ 5 Module (benchmark.py 222Z, models.py 158Z, providers.py 329Z, output.py 181Z, prompts.py 313Z)
- [x] Fixes: REQUEST_DELAY aus Semaphore verschoben, 1Ã— Retry bei HTTP 429 eingebaut

### Phase 2: Quelldokumente âœ“
- [x] BCG AI Radar 2026 â†’ `documents/pdf_files/ai-radar-2026-web-jan-2026-edit.pdf`
- [x] Alan Turing AI Regulatory Framework â†’ `documents/pdf_files/alan_turing_the_ai_regulatory.pdf`
- [x] EU AI Act (deutsch, 144S.) â†’ `documents/pdf_files/EU_AI_ACT_DE_TXT.pdf`
- [x] EVN GeschÃ¤ftsbericht 2024/25 â†’ `documents/pdf_files/EVN-GHB-2024-25_online.pdf`
- [x] Kuratierte Extrakte fÃ¼r A5 â†’ `documents/extracts/` (EU_AI_ACT_Art4_extract.txt, turing_framework_extract.txt)
- [x] `generate_extracts.py` erstellt â†’ MUSS EINMAL AUSGEFÃœHRT WERDEN: `cd Benchmark_Test && python generate_extracts.py`
- [x] Chunk-Strategie implementiert: Beide A5-Varianten nutzen Extrakte (~4.4k Tokens statt ~108k)
- [x] prompts.py v3.2: Alle doc-Referenzen auf tatsÃ¤chliche Dateinamen aktualisiert
- [x] Skill entpackt â†’ `skills/benchmark-specs/` (SKILL.md, references/, benchmark.py, prompts.py Snapshot)
- [x] `generate_extracts.py` ausgefÃ¼hrt: EU-Extrakt 1.277 WÃ¶rter, Turing-Extrakt 2.533 WÃ¶rter, ~4.446 Tokens gesamt
- [x] Skill `benchmark-evaluator` erstellt (SKILL.md + references/scoring_criteria.md + references/methodology_references.md)
- [x] Skill `benchmark-specs` aktualisiert (Phase 2 DONE, Refactoring-Status, Verweis auf Evaluator)
- [-] Statistik Austria / Microsoft Work Trend Index â†’ nicht mehr benÃ¶tigt (A5 umdesignt auf UK vs. EU Regulierung)
- [x] A5 Dual-Input-Design: N-Variante nutzt volle PDFs (~108k Tokens), P-Variante nutzt kuratierte Extrakte (~4.4k Tokens)
- [x] Retry auf 3 mit exponentiellem Backoff (10s/30s/90s), auch HTTP 503/529
- [x] Dry-Run um Token-SchÃ¤tzung und Kontextfenster-Warnungen erweitert
- [x] Input-Tokens in AggregatedResult und aggregated_stats.csv aufgenommen
- [x] merge_runs.py erstellt: FÃ¼hrt separate run_*-Verzeichnisse zusammen

### Phase 3: Testdurchlauf ðŸŸ¡ IN PROGRESS
- [x] `.env` mit API-Keys konfiguriert
- [x] `python benchmark.py --dry-run` â€“ Token-SchÃ¤tzungen validiert:
  - A5_N: 128.350 Tokens (volle PDFs) â€“ grenzwertig fÃ¼r 128k-Modelle
  - A6_N/P: ~168k Tokens (voller EVN-Bericht) â€“ Ã¼berschreitet 128k-Modelle
- [x] Smoke Test: Claude Opus 4.6 Ã— A1_N â€“ erfolgreich (1.351 Tokens, 30s)
- [x] System-Prompt-Test: Claude Opus 4.6 Ã— A1_P â€“ FlieÃŸtext, Deutsch, keine KI-Referenz âœ“
- [ ] o1 System-Prompt-KompatibilitÃ¤t prÃ¼fen (wenn erste P-Ergebnisse vorliegen)
- [x] Google Gemini Flash Ã— A6: HTTP 429 Quota-Problem. Google-Modelle auf separaten Run verschoben.
- [x] Hauptlauf gestartet: 9 Modelle (Anthropic + OpenAI), 12 Tasks Ã— 10 Runs = 1.080 Requests
- [ ] Google-Run nachholen (3 Modelle), dann merge_runs.py
- [ ] Ergebnisse prÃ¼fen: Antworten, Statistik, Fehler-Report

### Phase 4: Voller Durchlauf â¬œ
- [ ] `python benchmark.py` â€“ 18 Modelle Ã— 12 Aufgaben Ã— 10 Runs
- [ ] Laufzeit und Kosten dokumentieren
- [ ] Konsistenz-Report prÃ¼fen (CV < 15% fÃ¼r alle Modelle)
- [ ] Provider-Summary prÃ¼fen (Direkt vs. OpenRouter)

### Phase 5: Bewertung â¬œ
- [ ] Bewertungs-Template (`bewertung_manual.csv`) ausfÃ¼llen
- [ ] Pro AufgabeÃ—Modell: Exemplarische Antwort (Median-Run) bewerten
- [ ] 5 Kriterien Ã— Gewichtung = gewichteter Score
- [ ] Leaderboard erstellen

### Phase 6: VerÃ¶ffentlichung â¬œ
- [ ] GitHub-Repo anlegen (Ã¶ffentlich)
- [ ] README finalisieren
- [ ] Ergebnisse als Leaderboard-Tabelle
- [ ] LinkedIn-Post: Benchmark-AnkÃ¼ndigung + Community-Aufruf
- [ ] GPT-5.3-Codex manuell nachtesten und ergÃ¤nzen

---

## 4. Offene Fragen

1. **Quartalsbericht:** Welches Unternehmen? Immofinanz passt zur Zielgruppe (Immobilien), Verbund wÃ¤re breiter.
2. **LLM-as-Judge:** Automatisierte ErsteinschÃ¤tzung als Vorfilter vor manueller Bewertung? Oder rein manuell?
3. **N-Variante Bewertung:** Gleiche Kriterien und Gewichtung fÃ¼r N und P? Oder separate Bewertung?
4. **GPT-5.3-Codex:** Wann API Ã¼ber OpenRouter verfÃ¼gbar? Manueller Test reicht erstmal.

---

## 5. Verworfene Alternativen (fÃ¼r Nachvollziehbarkeit)

| Idee | Warum verworfen |
|------|-----------------|
| Operative Aufgaben (E-Mail, Text verbessern) | Zu kleinteilig fÃ¼r Thoughtful-Leader-Positionierung |
| Nur OpenRouter | Teurer, Gerald hat bereits Abos |
| Nur 1 Run pro Modell | Statistisch zu schwach, keine Varianz-Messung |
| Automatische Bewertung (LLM-as-Judge) | Zu unsicher bei nuancierten Strategieantworten, offen als ErgÃ¤nzung |
| 30+ Modelle | Diminishing Returns, die wichtigsten 18 decken den Markt ab |
| Temperatur > 0 | Nicht reproduzierbar, macht Multi-Run-Vergleich schwieriger |
