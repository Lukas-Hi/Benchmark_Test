# Methodik: Entscheider-Benchmark
**HID-LINKEDIN-BENCHMARK-2026-02-06-ACTIVE-C4E8A1-CLO46**
Stand: 6. Februar 2026

---

## 1. Design-Prinzipien

Dieser Benchmark orientiert sich an etablierten Methodik-Vorbildern, adaptiert sie aber f√ºr einen spezifischen Anwendungsfall: strategische F√ºhrungsarbeit im KMU-Kontext.

### 1.1 Referenz-Benchmarks

**GDPval (OpenAI, Oktober 2025)**
- 44 Berufe, echte Arbeitsprodukte, Experten-Bewertung
- Grundsatz: Teste mit realen Aufgaben, nicht mit synthetischen
- √úbernommen: Praxisnahe Szenarien, standardisierte Bewertung, Limitationen transparent benennen
- Nicht √ºbernommen: Skalierung auf 1.320 Tasks (f√ºr unser Setting unrealistisch)

**Artificial Analysis Intelligence Index v4.0**
- >10 Wiederholungen pro Modell f√ºr 95%-Konfidenzintervall (¬±1%)
- √úbernommen: Multi-Run-Methodik mit Konsistenz-Messung
- Nicht √ºbernommen: Automatisierte Bewertung (bei nuancierten Strategieantworten unzureichend)

**BetterBench (Stanford, 2025)**
- Kritik an existierenden Benchmarks: fehlende Reproduzierbarkeit, Score-Inflation
- √úbernommen: Testbedingungen explizit dokumentieren, Temperatur fixieren
- Nicht √ºbernommen: Head-to-Head-Vergleich (wir nutzen absolute Scores)

**MLPerf Inference**
- Industriestandard f√ºr Hardware-Benchmarks, strenge Reproduzierbarkeit
- √úbernommen: Dokumentation aller Parameter, identische Bedingungen f√ºr alle Teilnehmer

### 1.2 Kernprinzipien

1. **Praxisn√§he:** Jede Aufgabe bildet eine reale Entscheidungssituation ab, wie sie ein GF im DACH-Raum kennt.
2. **Reproduzierbarkeit:** Temperatur 0, identische Prompts, dokumentierte Modellversionen. Jeder kann den Test wiederholen.
3. **Echte Daten:** Dokumentbasierte Aufgaben nutzen publizierte Berichte (BCG, Statistik Austria, Microsoft), keine synthetischen Texte.
4. **DACH-Relevanz:** Szenarien in Wien/Graz, √∂sterreichische Unternehmen, deutsche Sprache, EU-Regulatorik.
5. **Transparenz:** Alle Prompts, Bewertungskriterien, Limitationen und verworfene Alternativen werden ver√∂ffentlicht.

---

## 2. Testbedingungen

| Parameter | Wert | Begr√ºndung |
|-----------|------|------------|
| Sprache | Deutsch | Zielgruppe DACH, testet Sprachqualit√§t |
| Temperatur | 0 | Maximale Reproduzierbarkeit |
| Max Tokens | 4.096 | Ausreichend f√ºr 400‚Äì800 W√∂rter + Overhead |
| Runs pro Modell√óAufgabe | 10 | Statistisch belastbar (vgl. Artificial Analysis) |
| Parallele Requests | Max 3 | Rate-Limit-Schutz |
| Delay zwischen Requests | 2 Sekunden | Rate-Limit-Schutz |
| Timeout | 300 Sekunden | Gro√üz√ºgig f√ºr Reasoning-Modelle |
| Kontext | Neuer Chat pro Request | Keine Vorgeschichte, kein Memory |

### 2.1 Warum Temperatur 0?

Bei Temperatur 0 sollte ein Modell bei identischem Input identische Outputs liefern. In der Praxis variieren Antworten trotzdem leicht (GPU-Parallelismus, Batching, Provider-spezifisches Sampling). Die 10 Runs messen diese Restvarianz. Ein Coefficient of Variation (CV) unter 5% gilt als sehr konsistent, 5‚Äì15% als normal, √ºber 15% als instabil.

### 2.2 Warum kein Chat-Kontext?

Jeder Request ist ein isolierter, neuer Chat. Kein System-Prompt aus vorherigen Interaktionen, kein Memory, keine Tools. Das simuliert die Realit√§t: Ein GF √∂ffnet ChatGPT/Claude und stellt eine Frage. Die N-Variante (Normal-User) bildet genau das ab.

---

## 3. Das Dual-Prompt-Design

### 3.1 Rationale

Die zentrale Hypothese: **Prompt-Kompetenz ist der st√§rkste Hebel f√ºr Output-Qualit√§t ‚Äì st√§rker als die Modellwahl.**

Um das zu testen, bekommt jedes Modell jede Aufgabe in zwei Varianten:

**N (Normal-User):** Wie ein typischer Gesch√§ftsf√ºhrer die Frage stellen w√ºrde.
- Kein System-Prompt
- Keine Struktur im Prompt
- Nat√ºrliche Sprache, oft unvollst√§ndiger Kontext
- Implizite Erwartungen ("sag mir was ich wissen muss")

**P (Power-User):** Optimierter Prompt nach Prompt-Engineering-Prinzipien.
- Vollst√§ndiger System-Prompt (Rolle, Sprache, Format, Haltung, L√§nge)
- Strukturierter Aufgaben-Prompt: KONTEXT ‚Üí SITUATION ‚Üí AUFTRAG
- Explizite Teilaufgaben ("Erstens... Zweitens... Drittens...")
- Anti-People-Pleasing-Guardrails
- Trennungsanweisungen (Fakt vs. Einsch√§tzung)

### 3.2 Messbare Deltas

F√ºr jedes Modell ergeben sich drei Messwerte pro Aufgabe:
- **Score N:** Leistung bei normalem Prompt
- **Score P:** Leistung bei optimiertem Prompt
- **Delta (P‚ÄìN):** Wie viel holt Prompt-Kompetenz heraus

Ein hohes Delta bedeutet: Das Modell hat Potenzial, das ohne gute Prompts brachliegt.
Ein niedriges Delta bedeutet: Das Modell liefert auch bei schlechten Prompts gute Ergebnisse (oder ist generell schwach).

### 3.3 Content-Implikation

Das Delta ist die Story f√ºr LinkedIn:
- "Modell X verbessert sich um 40% wenn der Prompt stimmt"
- "Das teuerste Modell mit schlechtem Prompt schl√§gt das billigste mit gutem Prompt nicht"
- "Kompetenz schl√§gt Budget ‚Äì und hier sind die Zahlen"

---

## 4. Bewertungsmatrix

### 4.1 F√ºnf Kriterien

| Kriterium | Gewicht | Was wird gemessen |
|-----------|---------|-------------------|
| **Substanz** | 25% | Tiefe der Analyse, eigenst√§ndige Schl√ºsse, √ºber das Offensichtliche hinaus |
| **Pr√§zision** | 25% | Faktentreue, keine Halluzinationen, Fakt vs. Einsch√§tzung getrennt |
| **Praxistauglichkeit** | 20% | Direkt umsetzbar, kennt Entscheider-Realit√§t, konkret statt abstrakt |
| **Urteilskraft** | 20% | Benennt Risiken ungefragt, kein People-Pleasing, traut sich Widerspruch |
| **Sprachqualit√§t (DE)** | 10% | Nat√ºrliches Gesch√§ftsdeutsch, keine Anglizismen, DACH-tauglich |

### 4.2 Bewertungsskala (1‚Äì5)

| Score | Label | Beschreibung |
|-------|-------|-------------|
| 1 | Mangelhaft | Oberfl√§chlich, generisch, Faktenfehler, nicht umsetzbar |
| 2 | Schwach | Einige relevante Punkte, aber l√ºckenhaft oder unpr√§zise |
| 3 | Solide | Brauchbar, deckt Kernpunkte ab, keine groben Fehler |
| 4 | Gut | Tiefgehend, praxisnah, eigenst√§ndige Einordnung |
| 5 | Exzellent | √úberraschend gut, identifiziert nicht-offensichtliche Punkte, GF w√ºrde das sofort nutzen |

### 4.3 Gewichteter Score

```
Score_gewichtet = (Substanz √ó 0.25) + (Pr√§zision √ó 0.25) + 
                  (Praxistauglichkeit √ó 0.20) + (Urteilskraft √ó 0.20) + 
                  (Sprachqualit√§t √ó 0.10)
```

Maximal: 5,00 | Minimal: 1,00

### 4.4 Ergebnisklassen

| Score | Klasse | Bedeutung f√ºr einen Entscheider |
|-------|--------|--------------------------------|
| 4,5‚Äì5,0 | **Sparringspartner** | Kann als Denkpartner f√ºr strategische Entscheidungen dienen |
| 3,5‚Äì4,4 | **Qualifizierter Zuarbeiter** | Liefert brauchbare Vorarbeit, braucht aber F√ºhrung |
| 2,5‚Äì3,4 | **Flei√üiger Assistent** | Erledigt Routinearbeit, bei Strategie √ºberfordert |
| 1,0‚Äì2,4 | **Nicht empfehlenswert** | F√ºr Entscheider-Aufgaben ungeeignet |

---

## 5. Bewertungsprozess

### 5.1 Vorgehen

1. Pro Modell√óAufgabe: Den Median-Run (gemessen an Antwortl√§nge) als Bewertungsgrundlage nehmen
2. Antwort lesen und gegen die 5 Kriterien bewerten (1‚Äì5 pro Kriterium)
3. Gewichteten Score berechnen
4. Kurze Bewertungsnotiz (1‚Äì2 S√§tze: Was war gut, was war schwach)
5. Ergebnis in `bewertung_manual.csv` eintragen

### 5.2 Bias-Kontrolle

- **Reihenfolge-Bias:** Antworten nicht in der Reihenfolge der Modellliste bewerten, sondern randomisiert
- **Erwartungs-Bias:** Modellname ist sichtbar (Blind-Bewertung bei 12√ó18 Antworten unpraktikabel), aber bewusst dagegen steuern
- **Anker-Bias:** Nicht die erste Antwort als Ma√üstab nehmen. Alle Antworten einer Aufgabe erst lesen, dann bewerten.

### 5.3 Inter-Rater-Reliabilit√§t

Aktuell: Einzelbewerter (Gerald). Bei Community-Beitr√§gen: Mehrere Bewerter m√∂glich, aber kein standardisiertes Inter-Rater-Protokoll. Das ist eine explizite Limitation.

---

## 6. Statistische Auswertung

### 6.1 Konsistenz-Metriken (automatisch)

| Metrik | Formel | Interpretation |
|--------|--------|----------------|
| Antwortl√§nge √ò | mean(len(response)) √ºber 10 Runs | Produktivit√§t |
| Antwortl√§nge CV | stdev/mean √ó 100% | Konsistenz: üü¢ <5%, üü° 5‚Äì15%, üî¥ >15% |
| Output-Tokens √ò | mean(output_tokens) | Token-Effizienz |
| Latenz √ò | mean(latency_seconds) | Geschwindigkeit |

### 6.2 Bewertungs-Metriken (manuell)

| Metrik | Berechnung |
|--------|------------|
| Score pro Aufgabe | Gewichteter Durchschnitt der 5 Kriterien |
| Gesamt-Score N | Durchschnitt √ºber alle N-Aufgaben |
| Gesamt-Score P | Durchschnitt √ºber alle P-Aufgaben |
| Delta (P‚ÄìN) | Gesamt-Score P minus Gesamt-Score N |
| Leaderboard-Rang | Sortiert nach Gesamt-Score P (prim√§r), dann Delta |

---

## 7. Limitationen

Diese Limitationen werden im README und in jeder Ver√∂ffentlichung transparent benannt:

1. **Kleine Aufgabenzahl:** 6 Aufgabenkategorien (12 mit N/P) decken nicht alle F√ºhrungsaufgaben ab. Repr√§sentativit√§t ist begrenzt.
2. **Einzelbewerter:** Manuelle Bewertung durch eine Person. Subjektivit√§t ist unvermeidbar. Inter-Rater-Reliabilit√§t nicht gemessen.
3. **DACH-Fokus:** Szenarien, Sprache und Regulatorik sind auf den deutschsprachigen Raum zugeschnitten. Ergebnisse sind nicht global √ºbertragbar.
4. **Statischer Zeitpunkt:** Benchmark zeigt Leistung zum Testzeitpunkt. Modelle werden laufend aktualisiert.
5. **Keine Kosten-Normalisierung:** Ein Modell das 10√ó teurer ist und 5% besser performt, erscheint im Leaderboard gleichwertig.
6. **Keine Tool-Use:** Getestet wird reines Text-in/Text-out. Web-Search, Code-Ausf√ºhrung, Datei-Analyse sind deaktiviert.
7. **Temperatur-0-Artefakte:** Bei Temperatur 0 k√∂nnen einige Modelle in Wiederholungsschleifen geraten oder degenerierte Outputs produzieren.
8. **PDF-Extraktion:** Dokumentbasierte Aufgaben h√§ngen von der Qualit√§t der Text-Extraktion ab (pdfplumber). Tabellen und Grafiken gehen m√∂glicherweise verloren.
